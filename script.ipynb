{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0385f08-033b-400f-b49b-f84a17424e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/reader/workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              file_name  page_number  chunk_number  \\\n",
      "0  Dataset summaries and citations.docx            1             1   \n",
      "1  Dataset summaries and citations.docx            1             2   \n",
      "2  Dataset summaries and citations.docx            1             3   \n",
      "3  Dataset summaries and citations.docx            1             4   \n",
      "4    Ocean_ecogeochemistry_A_review.pdf            1             1   \n",
      "\n",
      "                                                text  \n",
      "0  Table 1. Description of studies included in th...  \n",
      "1  bock, Texas. Agronomy Journal, 112(1), 148‚Äì157...  \n",
      "2  2010). Soil Organic Carbon Input from Urban Tu...  \n",
      "3   (2018). Soil carbon and nitrogen accumulation...  \n",
      "4  327\\nOceanography and Marine Biology: An Annua...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import csv\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens_per_chunk = 500\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return [page.get_text() for page in doc]\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return [\"\\n\".join(para.text for para in doc.paragraphs)]\n",
    "\n",
    "def extract_text_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [f.read()]\n",
    "\n",
    "def extract_text_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "    pages = []\n",
    "    for sheet, data in df.items():\n",
    "        pages.append(f\"Sheet: {sheet}\\n\" + data.to_string(index=False))\n",
    "    return pages\n",
    "\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\".csv\"]:\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\", \".xlsm\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return []\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_chunks = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pages = extract_text(file_path)\n",
    "            for page_num, text in enumerate(pages, 1):\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk_num, chunk in enumerate(chunks, 1):\n",
    "                    all_chunks.append({\n",
    "                        \"file_name\": filename,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_number\": chunk_num,\n",
    "                        \"text\": chunk\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "\n",
    "# üîç Usage\n",
    "folder_path = \"/Users/yasir/Desktop/Project/Dr.X Files\"  # or your local folder path\n",
    "df = process_folder(folder_path)\n",
    "print(df.head())\n",
    "df.to_csv(\"all_chunked_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98dbf52-d72c-45a7-8b36-534b01fc1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load once\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts_local(texts):\n",
    "    \"\"\"Embed text using a local SentenceTransformer model.\"\"\"\n",
    "    return embed_model.encode(texts).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efe11e6-b680-4ae7-9fcf-85c9b73bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"drx_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}: {e}\")\n",
    "    print(\"‚úÖ Chunks stored in ChromaDB using local embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f891cb9b-ddab-43f4-ac33-544d32928342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_local(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        # print(f\"üìÑ {meta['file_name']} (Page {meta['page']}, Chunk {meta['chunk']})\")\n",
    "        print(doc)\n",
    "        # print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447aa36d-b33f-42ee-ba07-b1c8a1a79cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " disappointment, so I prefer just to\n",
      "dream about it.‚Äù\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_chromadb_local(\"following your dream is like\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
