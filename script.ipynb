{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0385f08-033b-400f-b49b-f84a17424e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/reader/workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "📄 Processing files: 100%|██████████████████████| 10/10 [00:00<00:00, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              file_name  page_number  chunk_number  \\\n",
      "0  Dataset summaries and citations.docx            1             1   \n",
      "1  Dataset summaries and citations.docx            1             2   \n",
      "2  Dataset summaries and citations.docx            1             3   \n",
      "3  Dataset summaries and citations.docx            1             4   \n",
      "4    Ocean_ecogeochemistry_A_review.pdf            1             1   \n",
      "\n",
      "                                                text  \n",
      "0  Table 1. Description of studies included in th...  \n",
      "1  bock, Texas. Agronomy Journal, 112(1), 148–157...  \n",
      "2  2010). Soil Organic Carbon Input from Urban Tu...  \n",
      "3   (2018). Soil carbon and nitrogen accumulation...  \n",
      "4  327\\nOceanography and Marine Biology: An Annua...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import csv\n",
    "from tqdm import tqdm  # 👈 import tqdm\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens_per_chunk = 500\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return [page.get_text() for page in doc]\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return [\"\\n\".join(para.text for para in doc.paragraphs)]\n",
    "\n",
    "def extract_text_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [f.read()]\n",
    "\n",
    "def extract_text_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "    pages = []\n",
    "    for sheet, data in df.items():\n",
    "        pages.append(f\"Sheet: {sheet}\\n\" + data.to_string(index=False))\n",
    "    return pages\n",
    "\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\".csv\"]:\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\", \".xlsm\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return []\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_chunks = []\n",
    "    file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    for filename in tqdm(file_list, desc=\"📄 Processing files\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            pages = extract_text(file_path)\n",
    "            for page_num, text in enumerate(pages, 1):\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk_num, chunk in enumerate(chunks, 1):\n",
    "                    all_chunks.append({\n",
    "                        \"file_name\": filename,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_number\": chunk_num,\n",
    "                        \"text\": chunk\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "\n",
    "# 🔍 Usage\n",
    "folder_path = \"/Users/yasir/Desktop/Project/Dr.X Files\"\n",
    "df = process_folder(folder_path)\n",
    "print(df.head())\n",
    "df.to_csv(\"all_chunked_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10b4fee-5115-485c-b334-1975a48391b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import ollama\n",
    "import re\n",
    "from tqdm import tqdm  # ✅ Added tqdm for progress bars\n",
    "\n",
    "# ========== Embedding Setup ==========\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "performance_log = []\n",
    "\n",
    "# ========== Performance Logger ==========\n",
    "def log_performance(stage, text_or_token_list, start_time, end_time):\n",
    "    if isinstance(text_or_token_list, list) and isinstance(text_or_token_list[0], int):\n",
    "        total_tokens = len(text_or_token_list)\n",
    "    else:\n",
    "        total_tokens = len(tokenizer.encode(text_or_token_list))\n",
    "\n",
    "    elapsed = end_time - start_time\n",
    "    tokens_per_sec = total_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "    log_entry = {\n",
    "        \"stage\": stage,\n",
    "        \"tokens\": total_tokens,\n",
    "        \"duration_sec\": round(elapsed, 4),\n",
    "        \"tokens_per_sec\": round(tokens_per_sec, 2),\n",
    "        \"timestamp\": pd.Timestamp.now()\n",
    "    }\n",
    "\n",
    "    performance_log.append(log_entry)\n",
    "    print(f\"📊 [{stage}] {total_tokens} tokens in {elapsed:.2f}s → {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "    return tokens_per_sec\n",
    "\n",
    "def export_log(filename=\"performance_log.csv\"):\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ Performance log saved to {filename}\")\n",
    "\n",
    "def summarize_performance():\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    if df.empty:\n",
    "        print(\"⚠️ No performance data to summarize.\")\n",
    "        return\n",
    "    summary = df.groupby(\"stage\")[\"tokens_per_sec\"].agg([\"count\", \"min\", \"max\", \"mean\"]).reset_index()\n",
    "    print(\"\\n📈 Performance Summary:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    return summary\n",
    "\n",
    "# ========== Preprocessing Helper ==========\n",
    "def remove_tables_and_numeric_lines(text):\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip().lower()\n",
    "\n",
    "        numbers = re.findall(r\"\\d\", line)\n",
    "        if len(numbers) / max(len(line), 1) > 0.4:\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"\\t\", line) or re.search(r\"\\|\", line):\n",
    "            continue\n",
    "        if len(re.findall(r\"\\s{2,}\", line)) > 2:\n",
    "            continue\n",
    "\n",
    "        if any(keyword in stripped for keyword in [\"author\", \"editor\", \"publisher\", \"published by\", \"phone\", \"email\", \"contact\", \"address\"]):\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"\\b\\w+@\\w+\\.\\w+\\b\", line):\n",
    "            continue\n",
    "        if re.search(r\"\\+?\\d[\\d\\s().-]{7,}\\d\", line):\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "# ========== Embedding Function ==========\n",
    "def embed_texts_local(texts):\n",
    "    start = time.time()\n",
    "    embeddings = embed_model.encode(texts).tolist()\n",
    "    end = time.time()\n",
    "    log_performance(\"embedding\", \" \".join(texts), start, end)\n",
    "    return embeddings\n",
    "\n",
    "# ========== Store to ChromaDB ==========\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"🔁 Storing in ChromaDB\"):\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}: {e}\")\n",
    "    print(\"✅ Chunks stored in ChromaDB using local embeddings.\")\n",
    "\n",
    "# ========== Ask Mistral via Ollama ==========\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"\"Write a clear, formal, human-like summary in academic tone.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    start = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    end = time.time()\n",
    "    log_performance(\"RAG\", prompt, start, end)\n",
    "    return response['message']['content']\n",
    "\n",
    "# ========== RAG Pipeline ==========\n",
    "def rag_query(question, collection_name=\"docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    return ask_mistral(question, context)\n",
    "\n",
    "# ========== Summarize All Documents ==========\n",
    "def summarize_all_documents(collection_name=\"docs\", persist_dir=\"./chroma_storage\"):\n",
    "    \"\"\"\n",
    "    Summarize all chunks grouped by document ID.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    summaries_by_file = {}\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "        file_name = meta.get(\"file_name\", \"unknown\")\n",
    "        text = doc if isinstance(doc, str) else doc[0]\n",
    "        if file_name not in summaries_by_file:\n",
    "            summaries_by_file[file_name] = []\n",
    "        summaries_by_file[file_name].append(text)\n",
    "\n",
    "    final_summaries = {}\n",
    "    for file, chunks in tqdm(list(summaries_by_file.items())[:1], desc=\"🧠 Summarizing files\"):\n",
    "        combined_text = \"\\n\\n\".join(chunks)\n",
    "        summary = ask_mistral(\"Summarize all the chunks of this document.\", combined_text)\n",
    "        final_summaries[file] = summary\n",
    "\n",
    "    return final_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9714b028-beec-4c76-83e0-b530ee029bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ========== Evaluate Summaries with ROUGE ==========\n",
    "def evaluate_summaries_with_rouge(generated_summaries: dict, reference_summaries: dict):\n",
    "    \"\"\"\n",
    "    Compare generated summaries to reference summaries using ROUGE scores.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    evaluation = {}\n",
    "\n",
    "    for doc_id, generated in tqdm(generated_summaries.items(), desc=\"📏 Evaluating ROUGE\"):\n",
    "        reference = reference_summaries.get(doc_id, \"\")\n",
    "        if not reference:\n",
    "            print(f\"⚠️ No reference summary found for document: {doc_id}\")\n",
    "            continue\n",
    "\n",
    "        scores = scorer.score(generated, reference)\n",
    "        evaluation[doc_id] = {\n",
    "            metric: {\n",
    "                \"precision\": round(scores[metric].precision, 4),\n",
    "                \"recall\": round(scores[metric].recall, 4),\n",
    "                \"f1\": round(scores[metric].fmeasure, 4)\n",
    "            } for metric in scores\n",
    "        }\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f06998-7317-427b-b29c-b89aec72bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summarize_all_documents()\n",
    "# response = rag_query(\"Tell me about Emotional Intelligence\")\n",
    "\n",
    "print(\"🤖 Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a410b7b-45c4-46e3-839a-e99cf7cef764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Summarizing files: 100%|███████████████████████| 1/1 [01:35<00:00, 95.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 [RAG] 1865 tokens in 95.93s → 19.44 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📏 Evaluating ROUGE: 100%|████████████████████████| 1/1 [00:00<00:00, 33.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataset summaries and citations.docx': {'rouge1': {'precision': 0.7137, 'recall': 0.3368, 'f1': 0.4576}, 'rouge2': {'precision': 0.2743, 'recall': 0.1292, 'f1': 0.1756}, 'rougeL': {'precision': 0.3524, 'recall': 0.1663, 'f1': 0.226}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Preloaded Reference Summaries ==========\n",
    "reference_summaries = {\n",
    "    \"Dataset summaries and citations.docx\": \"\"\"\n",
    "    1. Numerous studies have investigated carbon accumulation and nitrogen cycling in various urban land uses including residential soils, golf courses, and home lawns.\n",
    "    2. Some research emphasizes specific biophysical factors influencing soil carbon, such as:\n",
    "       - Microbial processes (e.g., Shi et al., 2012),\n",
    "       - Management intensity and duration (e.g., Wang et al., 2014),\n",
    "       - Historical land use changes (e.g., Raccanello et al., 2011).\n",
    "    3. A subset of studies analyzes regional influences on carbon sequestration, examining how soil characteristics and climatic conditions affect carbon dynamics in urban ecosystems (e.g., Selhorst & Lal, 2011; Smith et al., 2018).\n",
    "    4. Comparative assessments have been conducted to evaluate differences in carbon sequestration across urban land typologies, including turfgrass systems and residential zones with varied development histories.\n",
    "    5. Findings indicate that urban landscapes can serve as significant carbon sinks, occasionally surpassing the sequestration potential of rural or forested lands (Selhorst & Lal, 2011; Trammell et al., 2020).\n",
    "    6. Despite their potential benefits, urban environments also contribute to greenhouse gas emissions, especially from impervious surfaces and vehicular traffic (Townsend-Small & Czimczik, 2010).\n",
    "    7. Research suggests that residential lawns have a variable carbon sequestration potential, heavily influenced by climate, management practices, and underlying soil properties.\n",
    "    8. In conclusion, while urban landscapes offer valuable opportunities for climate change mitigation through carbon storage, maximizing their potential requires integrated land management strategies that simultaneously limit greenhouse gas emissions.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# ========== Example Evaluation Run ==========\n",
    "generated_summaries = summarize_all_documents()\n",
    "results = evaluate_summaries_with_rouge(generated_summaries, reference_summaries)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11a8ab-090c-40b0-a7d4-006bc40ccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_documents(collection_name=\"docs\", persist_dir=\"./chroma_storage\", max_chunks=5):\n",
    "    \"\"\"\n",
    "    Summarize a limited number of stored chunks to reduce load and return a list of summaries.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\"])\n",
    "    all_chunks = results.get(\"documents\", [])\n",
    "\n",
    "    print(f\"🧠 Summarizing up to {max_chunks} chunks...\")\n",
    "    summaries = []\n",
    "    for idx, chunk in enumerate(all_chunks[:max_chunks]):\n",
    "        if isinstance(chunk, str) and chunk.strip():\n",
    "            summary = ask_mistral(\"Summarize the following document chunk.\", chunk)\n",
    "            summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dbf52-d72c-45a7-8b36-534b01fc1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load once\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts_local(texts):\n",
    "    \"\"\"Embed text using a local SentenceTransformer model.\"\"\"\n",
    "    return embed_model.encode(texts).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe11e6-b680-4ae7-9fcf-85c9b73bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"drx_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}: {e}\")\n",
    "    print(\"✅ Chunks stored in ChromaDB using local embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891cb9b-ddab-43f4-ac33-544d32928342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_local(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "    # print(results)\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"📄 {meta['file_name']} (Page {meta['page']}, Chunk {meta['chunk']})\")\n",
    "        print(doc)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414e3b2-3f97-4775-ad88-fff8d24ad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072137e2-017a-41ee-ab51-c3731bf3f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=5):\n",
    "    # Embed the question\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    # Query ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    # Concatenate top chunks\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Generate answer with Ollama Mistral\n",
    "    answer = ask_mistral(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447aa36d-b33f-42ee-ba07-b1c8a1a79cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_query(\"summarize all papers?\")\n",
    "print(\"🤖 Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5da8b2-4504-41a3-bfcf-827d526c16a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca069ad9-dfef-4dde-9c41-51e71e92e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003d2f6-a141-41e4-a1d6-12c33842a401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb892274-b3c5-49f5-899c-96aad5d43917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
