{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0385f08-033b-400f-b49b-f84a17424e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/reader/workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              file_name  page_number  chunk_number  \\\n",
      "0  Dataset summaries and citations.docx            1             1   \n",
      "1  Dataset summaries and citations.docx            1             2   \n",
      "2  Dataset summaries and citations.docx            1             3   \n",
      "3  Dataset summaries and citations.docx            1             4   \n",
      "4    Ocean_ecogeochemistry_A_review.pdf            1             1   \n",
      "\n",
      "                                                text  \n",
      "0  Table 1. Description of studies included in th...  \n",
      "1  bock, Texas. Agronomy Journal, 112(1), 148‚Äì157...  \n",
      "2  2010). Soil Organic Carbon Input from Urban Tu...  \n",
      "3   (2018). Soil carbon and nitrogen accumulation...  \n",
      "4  327\\nOceanography and Marine Biology: An Annua...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import csv\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens_per_chunk = 500\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return [page.get_text() for page in doc]\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return [\"\\n\".join(para.text for para in doc.paragraphs)]\n",
    "\n",
    "def extract_text_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [f.read()]\n",
    "\n",
    "def extract_text_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "    pages = []\n",
    "    for sheet, data in df.items():\n",
    "        pages.append(f\"Sheet: {sheet}\\n\" + data.to_string(index=False))\n",
    "    return pages\n",
    "\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\".csv\"]:\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\", \".xlsm\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return []\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_chunks = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pages = extract_text(file_path)\n",
    "            for page_num, text in enumerate(pages, 1):\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk_num, chunk in enumerate(chunks, 1):\n",
    "                    all_chunks.append({\n",
    "                        \"file_name\": filename,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_number\": chunk_num,\n",
    "                        \"text\": chunk\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "\n",
    "# üîç Usage\n",
    "folder_path = \"/Users/yasir/Desktop/Project/Dr.X Files\"  # or your local folder path\n",
    "df = process_folder(folder_path)\n",
    "print(df.head())\n",
    "df.to_csv(\"all_chunked_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10b4fee-5115-485c-b334-1975a48391b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import ollama\n",
    "import re\n",
    "\n",
    "\n",
    "# ========== Embedding Setup ==========\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "performance_log = []\n",
    "\n",
    "# ========== Performance Logger ==========\n",
    "def log_performance(stage, text_or_token_list, start_time, end_time):\n",
    "    if isinstance(text_or_token_list, list) and isinstance(text_or_token_list[0], int):\n",
    "        total_tokens = len(text_or_token_list)\n",
    "    else:\n",
    "        total_tokens = len(tokenizer.encode(text_or_token_list))\n",
    "\n",
    "    elapsed = end_time - start_time\n",
    "    tokens_per_sec = total_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "    log_entry = {\n",
    "        \"stage\": stage,\n",
    "        \"tokens\": total_tokens,\n",
    "        \"duration_sec\": round(elapsed, 4),\n",
    "        \"tokens_per_sec\": round(tokens_per_sec, 2),\n",
    "        \"timestamp\": pd.Timestamp.now()\n",
    "    }\n",
    "\n",
    "    performance_log.append(log_entry)\n",
    "    print(f\"üìä [{stage}] {total_tokens} tokens in {elapsed:.2f}s ‚Üí {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "    return tokens_per_sec\n",
    "\n",
    "def export_log(filename=\"performance_log.csv\"):\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Performance log saved to {filename}\")\n",
    "\n",
    "def summarize_performance():\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No performance data to summarize.\")\n",
    "        return\n",
    "    summary = df.groupby(\"stage\")[\"tokens_per_sec\"].agg([\"count\", \"min\", \"max\", \"mean\"]).reset_index()\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    return summary\n",
    "\n",
    "# ========== Preprocessing Helper ==========\n",
    "def remove_tables_and_numeric_lines(text):\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip().lower()\n",
    "\n",
    "        # Remove if mostly numbers or numeric-dominant\n",
    "        numbers = re.findall(r\"\\d\", line)\n",
    "        if len(numbers) / max(len(line), 1) > 0.4:\n",
    "            continue\n",
    "\n",
    "        # Remove likely table rows: many tabs or pipes or columns separated by spaces\n",
    "        if re.search(r\"\\t\", line) or re.search(r\"\\|\", line):\n",
    "            continue\n",
    "        if len(re.findall(r\"\\s{2,}\", line)) > 2:  # lines with multiple columns spaced apart\n",
    "            continue\n",
    "\n",
    "        # Remove author info, emails, phone numbers, addresses\n",
    "        if any(keyword in stripped for keyword in [\"author\", \"editor\", \"publisher\", \"published by\", \"phone\", \"email\", \"contact\", \"address\"]):\n",
    "            continue\n",
    "\n",
    "        # Remove lines with emails or phone numbers\n",
    "        if re.search(r\"\\b\\w+@\\w+\\.\\w+\\b\", line):\n",
    "            continue\n",
    "        if re.search(r\"\\+?\\d[\\d\\s().-]{7,}\\d\", line):\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered_lines)\n",
    "# ========== Embedding Function ==========\n",
    "def embed_texts_local(texts):\n",
    "    start = time.time()\n",
    "    embeddings = embed_model.encode(texts).tolist()\n",
    "    end = time.time()\n",
    "    log_performance(\"embedding\", \" \".join(texts), start, end)\n",
    "    return embeddings\n",
    "\n",
    "# ========== Store to ChromaDB ==========\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}: {e}\")\n",
    "    print(\"‚úÖ Chunks stored in ChromaDB using local embeddings.\")\n",
    "\n",
    "# ========== Ask Mistral via Ollama ==========\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    start = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    end = time.time()\n",
    "    log_performance(\"RAG\", prompt, start, end)\n",
    "    return response['message']['content']\n",
    "\n",
    "# ========== RAG Pipeline ==========\n",
    "def rag_query(question, collection_name=\"docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    return ask_mistral(question, context)\n",
    "    \n",
    "def summarize_all_documents(collection_name=\"docs\", persist_dir=\"./chroma_storage\"):\n",
    "    \"\"\"\n",
    "    Summarize all chunks grouped by document ID.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    summaries_by_file = {}\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "        file_name = meta.get(\"file_name\", \"unknown\")\n",
    "        text = doc if isinstance(doc, str) else doc[0]\n",
    "        if file_name not in summaries_by_file:\n",
    "            summaries_by_file[file_name] = []\n",
    "        summaries_by_file[file_name].append(text)\n",
    "\n",
    "    final_summaries = {}\n",
    "    for file, chunks in summaries_by_file.items():\n",
    "        combined_text = \"\\n\\n\".join(chunks)\n",
    "        summary = ask_mistral(\"Summarize all the chunks of this document.\", combined_text)\n",
    "        print(f\"The Summary of {file} is: \\n{summary}\")\n",
    "        final_summaries[file] = summary\n",
    "\n",
    "    return final_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f06998-7317-427b-b29c-b89aec72bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [embedding] 5 tokens in 0.53s ‚Üí 9.48 tokens/sec\n",
      "üìä [RAG] 517 tokens in 48.62s ‚Üí 10.63 tokens/sec\n",
      "ü§ñ Mistral says:\n",
      "  Emotional Intelligence (EI), also known as emotional quotient (EQ), refers to an individual's ability to recognize, understand, and manage their own emotions, as well as the emotions of others. It is a set of skills that complements academic intelligence (IQ). The concept has been explored extensively in various books such as \"Agility: How Rapid Learning Will Open Up the Future\" by Dan Goleman, \"The Brain and Emotional Intelligence: New Insights\" by Dan Goleman, and \"The Language of Emotional Intelligence\" by Jean Segal.\n",
      "\n",
      "In simpler terms, EI is about being aware of our emotions and how they affect our thoughts and actions, as well as the emotions of others and how to respond effectively. It includes five key components: self-awareness, self-regulation, motivation, empathy, and social skills.\n",
      "\n",
      "To measure one's emotional intelligence, various tools can be used. These range from self-report questionnaires like the Emotional Quotient Inventory (EQ-i) or the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT), to performance-based assessments like the Movie Clips Protocol (MCP).\n",
      "\n",
      "The science behind emotional intelligence revolves around neuroscience, psychology, and sociology. It is based on the idea that emotional skills can be learned and improved over time. EI has been linked with various positive outcomes such as better mental health, stronger relationships, and higher levels of success in both personal and professional life.\n",
      "\n",
      "In terms of self-study, one can explore online courses on platforms like Coursera (e.g., \"Emotional Intelligence for Leadership Development\") or Positive Psychology Program, which offer comprehensive programs on emotional intelligence. Additionally, reading books like those mentioned earlier can provide valuable insights into the concept and its practical applications.\n",
      "\n",
      "Lastly, in the context of Computer Assisted Research Skills, EI can be relevant as it involves understanding data (empathy with data), interpreting results (self-awareness and self-regulation), presenting findings effectively (social skills and empathy for audience), and making decisions based on evidence (motivation). Therefore, learning emotional intelligence can complement the skills gained in this course.\n"
     ]
    }
   ],
   "source": [
    "# response = summarize_all_documents()\n",
    "response = rag_query(\"Tell me about Emotional Intelligence\")\n",
    "\n",
    "print(\"ü§ñ Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11a8ab-090c-40b0-a7d4-006bc40ccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_documents(collection_name=\"docs\", persist_dir=\"./chroma_storage\", max_chunks=5):\n",
    "    \"\"\"\n",
    "    Summarize a limited number of stored chunks to reduce load and return a list of summaries.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\"])\n",
    "    all_chunks = results.get(\"documents\", [])\n",
    "\n",
    "    print(f\"üß† Summarizing up to {max_chunks} chunks...\")\n",
    "    summaries = []\n",
    "    for idx, chunk in enumerate(all_chunks[:max_chunks]):\n",
    "        if isinstance(chunk, str) and chunk.strip():\n",
    "            summary = ask_mistral(\"Summarize the following document chunk.\", chunk)\n",
    "            summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dbf52-d72c-45a7-8b36-534b01fc1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load once\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts_local(texts):\n",
    "    \"\"\"Embed text using a local SentenceTransformer model.\"\"\"\n",
    "    return embed_model.encode(texts).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe11e6-b680-4ae7-9fcf-85c9b73bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"drx_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}: {e}\")\n",
    "    print(\"‚úÖ Chunks stored in ChromaDB using local embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891cb9b-ddab-43f4-ac33-544d32928342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_local(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "    # print(results)\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"üìÑ {meta['file_name']} (Page {meta['page']}, Chunk {meta['chunk']})\")\n",
    "        print(doc)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414e3b2-3f97-4775-ad88-fff8d24ad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072137e2-017a-41ee-ab51-c3731bf3f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=5):\n",
    "    # Embed the question\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    # Query ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    # Concatenate top chunks\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Generate answer with Ollama Mistral\n",
    "    answer = ask_mistral(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447aa36d-b33f-42ee-ba07-b1c8a1a79cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_query(\"summarize all papers?\")\n",
    "print(\"ü§ñ Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5da8b2-4504-41a3-bfcf-827d526c16a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca069ad9-dfef-4dde-9c41-51e71e92e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003d2f6-a141-41e4-a1d6-12c33842a401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb892274-b3c5-49f5-899c-96aad5d43917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
