{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0385f08-033b-400f-b49b-f84a17424e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/reader/workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              file_name  page_number  chunk_number  \\\n",
      "0  Dataset summaries and citations.docx            1             1   \n",
      "1  Dataset summaries and citations.docx            1             2   \n",
      "2  Dataset summaries and citations.docx            1             3   \n",
      "3  Dataset summaries and citations.docx            1             4   \n",
      "4    Ocean_ecogeochemistry_A_review.pdf            1             1   \n",
      "\n",
      "                                                text  \n",
      "0  Table 1. Description of studies included in th...  \n",
      "1  bock, Texas. Agronomy Journal, 112(1), 148–157...  \n",
      "2  2010). Soil Organic Carbon Input from Urban Tu...  \n",
      "3   (2018). Soil carbon and nitrogen accumulation...  \n",
      "4  327\\nOceanography and Marine Biology: An Annua...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import csv\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens_per_chunk = 500\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return [page.get_text() for page in doc]\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return [\"\\n\".join(para.text for para in doc.paragraphs)]\n",
    "\n",
    "def extract_text_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [f.read()]\n",
    "\n",
    "def extract_text_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "    pages = []\n",
    "    for sheet, data in df.items():\n",
    "        pages.append(f\"Sheet: {sheet}\\n\" + data.to_string(index=False))\n",
    "    return pages\n",
    "\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\".csv\"]:\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\", \".xlsm\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return []\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_chunks = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pages = extract_text(file_path)\n",
    "            for page_num, text in enumerate(pages, 1):\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk_num, chunk in enumerate(chunks, 1):\n",
    "                    all_chunks.append({\n",
    "                        \"file_name\": filename,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_number\": chunk_num,\n",
    "                        \"text\": chunk\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "\n",
    "# 🔍 Usage\n",
    "folder_path = \"/Users/yasir/Desktop/Project/Dr.X Files\"  # or your local folder path\n",
    "df = process_folder(folder_path)\n",
    "print(df.head())\n",
    "df.to_csv(\"all_chunked_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10b4fee-5115-485c-b334-1975a48391b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 [embedding] 500 tokens in 0.13s → 3798.59 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34050.76 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35203.65 tokens/sec\n",
      "📊 [embedding] 333 tokens in 0.01s → 25888.37 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38928.42 tokens/sec\n",
      "📊 [embedding] 249 tokens in 0.04s → 5739.21 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30051.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36352.72 tokens/sec\n",
      "📊 [embedding] 143 tokens in 0.04s → 3604.46 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34813.86 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34578.51 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33774.37 tokens/sec\n",
      "📊 [embedding] 3 tokens in 0.04s → 70.55 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34611.04 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31523.80 tokens/sec\n",
      "📊 [embedding] 185 tokens in 0.04s → 4197.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34328.89 tokens/sec\n",
      "📊 [embedding] 231 tokens in 0.04s → 5993.01 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33409.04 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36032.30 tokens/sec\n",
      "📊 [embedding] 163 tokens in 0.04s → 4395.78 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32844.98 tokens/sec\n",
      "📊 [embedding] 368 tokens in 0.01s → 28828.45 tokens/sec\n",
      "📊 [embedding] 318 tokens in 0.04s → 7766.51 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36816.04 tokens/sec\n",
      "📊 [embedding] 369 tokens in 0.01s → 26989.71 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39400.89 tokens/sec\n",
      "📊 [embedding] 369 tokens in 0.01s → 27677.01 tokens/sec\n",
      "📊 [embedding] 290 tokens in 0.04s → 7134.26 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38393.91 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34329.45 tokens/sec\n",
      "📊 [embedding] 90 tokens in 0.04s → 2358.06 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40833.19 tokens/sec\n",
      "📊 [embedding] 297 tokens in 0.01s → 23418.65 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38895.21 tokens/sec\n",
      "📊 [embedding] 325 tokens in 0.01s → 25388.78 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37072.46 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33339.99 tokens/sec\n",
      "📊 [embedding] 1 tokens in 0.05s → 21.38 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33367.04 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37235.70 tokens/sec\n",
      "📊 [embedding] 63 tokens in 0.04s → 1576.03 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33524.93 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33156.03 tokens/sec\n",
      "📊 [embedding] 193 tokens in 0.04s → 5105.94 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32807.98 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33066.11 tokens/sec\n",
      "📊 [embedding] 37 tokens in 0.04s → 838.39 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33031.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34151.12 tokens/sec\n",
      "📊 [embedding] 169 tokens in 0.04s → 4449.84 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32285.23 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39105.54 tokens/sec\n",
      "📊 [embedding] 223 tokens in 0.04s → 5597.53 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33590.44 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35325.81 tokens/sec\n",
      "📊 [embedding] 58 tokens in 0.05s → 1233.12 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32690.36 tokens/sec\n",
      "📊 [embedding] 302 tokens in 0.01s → 20896.10 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36756.03 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36886.62 tokens/sec\n",
      "📊 [embedding] 174 tokens in 0.04s → 4224.21 tokens/sec\n",
      "📊 [embedding] 361 tokens in 0.02s → 22697.40 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34671.69 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37974.00 tokens/sec\n",
      "📊 [embedding] 112 tokens in 0.04s → 2550.50 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32541.23 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36303.63 tokens/sec\n",
      "📊 [embedding] 160 tokens in 0.04s → 3972.89 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32084.21 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33617.90 tokens/sec\n",
      "📊 [embedding] 106 tokens in 0.04s → 2582.16 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33868.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34174.49 tokens/sec\n",
      "📊 [embedding] 64 tokens in 0.05s → 1358.86 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33080.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37239.01 tokens/sec\n",
      "📊 [embedding] 75 tokens in 0.04s → 1740.38 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33522.79 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33311.39 tokens/sec\n",
      "📊 [embedding] 96 tokens in 0.05s → 2060.48 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32425.51 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36197.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37980.19 tokens/sec\n",
      "📊 [embedding] 23 tokens in 0.04s → 563.40 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33772.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35196.56 tokens/sec\n",
      "📊 [embedding] 493 tokens in 0.01s → 38001.80 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36902.85 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35033.11 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34797.11 tokens/sec\n",
      "📊 [embedding] 13 tokens in 0.05s → 288.11 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37399.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35948.30 tokens/sec\n",
      "📊 [embedding] 460 tokens in 0.01s → 32800.87 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38610.21 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37467.21 tokens/sec\n",
      "📊 [embedding] 368 tokens in 0.01s → 27860.07 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39538.32 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39176.40 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37878.66 tokens/sec\n",
      "📊 [embedding] 10 tokens in 0.05s → 215.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33849.60 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38413.60 tokens/sec\n",
      "📊 [embedding] 431 tokens in 0.01s → 31377.93 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35630.70 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37255.55 tokens/sec\n",
      "📊 [embedding] 408 tokens in 0.01s → 30242.04 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39236.51 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39347.67 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38482.68 tokens/sec\n",
      "📊 [embedding] 70 tokens in 0.04s → 1596.83 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33588.29 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33777.09 tokens/sec\n",
      "📊 [embedding] 446 tokens in 0.01s → 33450.63 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36827.03 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39053.11 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37285.35 tokens/sec\n",
      "📊 [embedding] 26 tokens in 0.01s → 3362.27 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35678.59 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33431.40 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31998.05 tokens/sec\n",
      "📊 [embedding] 92 tokens in 0.04s → 2245.23 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31385.56 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34506.25 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37023.37 tokens/sec\n",
      "📊 [embedding] 103 tokens in 0.01s → 12225.52 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36348.31 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35506.43 tokens/sec\n",
      "📊 [embedding] 405 tokens in 0.01s → 30066.96 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36797.30 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34048.54 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32614.10 tokens/sec\n",
      "📊 [embedding] 17 tokens in 0.04s → 418.60 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35561.83 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35333.55 tokens/sec\n",
      "📊 [embedding] 461 tokens in 0.01s → 33862.35 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35767.43 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36326.27 tokens/sec\n",
      "📊 [embedding] 277 tokens in 0.01s → 21137.49 tokens/sec\n",
      "📊 [embedding] 431 tokens in 0.01s → 29675.06 tokens/sec\n",
      "📊 [embedding] 323 tokens in 0.04s → 7517.55 tokens/sec\n",
      "📊 [embedding] 337 tokens in 0.04s → 8089.14 tokens/sec\n",
      "📊 [embedding] 211 tokens in 0.05s → 4423.11 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30938.29 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.05s → 9927.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35870.21 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34621.32 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.05s → 10364.91 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37739.60 tokens/sec\n",
      "📊 [embedding] 196 tokens in 0.05s → 4140.08 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31784.18 tokens/sec\n",
      "📊 [embedding] 197 tokens in 0.05s → 3893.00 tokens/sec\n",
      "📊 [embedding] 399 tokens in 0.01s → 28939.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33319.86 tokens/sec\n",
      "📊 [embedding] 267 tokens in 0.05s → 5796.27 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37806.96 tokens/sec\n",
      "📊 [embedding] 334 tokens in 0.01s → 28152.52 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40403.66 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37858.83 tokens/sec\n",
      "📊 [embedding] 119 tokens in 0.05s → 2247.71 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33552.28 tokens/sec\n",
      "📊 [embedding] 285 tokens in 0.01s → 20270.24 tokens/sec\n",
      "📊 [embedding] 457 tokens in 0.01s → 35011.27 tokens/sec\n",
      "📊 [embedding] 323 tokens in 0.01s → 25819.22 tokens/sec\n",
      "📊 [embedding] 497 tokens in 0.01s → 41249.19 tokens/sec\n",
      "📊 [embedding] 360 tokens in 0.01s → 29154.67 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36320.61 tokens/sec\n",
      "📊 [embedding] 199 tokens in 0.05s → 4247.95 tokens/sec\n",
      "📊 [embedding] 358 tokens in 0.02s → 23602.40 tokens/sec\n",
      "📊 [embedding] 266 tokens in 0.02s → 15456.12 tokens/sec\n",
      "📊 [embedding] 420 tokens in 0.01s → 31031.16 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38649.34 tokens/sec\n",
      "📊 [embedding] 238 tokens in 0.05s → 4851.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33127.75 tokens/sec\n",
      "📊 [embedding] 381 tokens in 0.01s → 27237.13 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34882.19 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33896.10 tokens/sec\n",
      "📊 [embedding] 497 tokens in 0.01s → 34937.89 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33690.27 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33339.99 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32994.84 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37475.91 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35099.95 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35288.36 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36132.87 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36526.20 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33832.13 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34867.69 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30665.50 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33898.30 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33020.82 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32729.64 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34146.67 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37283.37 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36512.21 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34587.06 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38223.86 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36445.59 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31204.83 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39188.12 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39476.55 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38735.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37383.05 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37675.87 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37196.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38567.60 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36195.24 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37238.35 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 26890.36 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36215.86 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34855.52 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33565.71 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34699.80 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36935.35 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34291.28 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33151.84 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36965.95 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40128.43 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32937.84 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38941.43 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40534.08 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40035.74 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38904.59 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40296.52 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37147.32 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36850.97 tokens/sec\n",
      "📊 [embedding] 18 tokens in 0.05s → 345.58 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33154.46 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32914.57 tokens/sec\n",
      "📊 [embedding] 100 tokens in 0.05s → 2120.90 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34273.91 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35206.02 tokens/sec\n",
      "📊 [embedding] 187 tokens in 0.01s → 18401.68 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36794.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38868.54 tokens/sec\n",
      "📊 [embedding] 251 tokens in 0.01s → 17869.61 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36260.95 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40199.20 tokens/sec\n",
      "📊 [embedding] 273 tokens in 0.05s → 5349.88 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 38494.69 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 42194.52 tokens/sec\n",
      "📊 [embedding] 412 tokens in 0.01s → 30207.55 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35924.90 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40631.46 tokens/sec\n",
      "📊 [embedding] 133 tokens in 0.05s → 2502.07 tokens/sec\n",
      "📊 [embedding] 464 tokens in 0.01s → 31537.14 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35448.22 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39386.09 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39056.02 tokens/sec\n",
      "📊 [embedding] 234 tokens in 0.05s → 4974.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31076.75 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32007.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30066.26 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33629.22 tokens/sec\n",
      "📊 [embedding] 88 tokens in 0.05s → 1835.28 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32412.48 tokens/sec\n",
      "📊 [embedding] 495 tokens in 0.01s → 33946.15 tokens/sec\n",
      "📊 [embedding] 67 tokens in 0.05s → 1342.18 tokens/sec\n",
      "📊 [embedding] 169 tokens in 0.05s → 3286.23 tokens/sec\n",
      "📊 [embedding] 341 tokens in 0.02s → 21572.51 tokens/sec\n",
      "📊 [embedding] 317 tokens in 0.02s → 20006.54 tokens/sec\n",
      "📊 [embedding] 220 tokens in 0.05s → 4304.28 tokens/sec\n",
      "📊 [embedding] 444 tokens in 0.01s → 32108.67 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 39379.44 tokens/sec\n",
      "📊 [embedding] 40 tokens in 0.05s → 748.46 tokens/sec\n",
      "📊 [embedding] 147 tokens in 0.05s → 2965.26 tokens/sec\n",
      "📊 [embedding] 362 tokens in 0.02s → 22728.59 tokens/sec\n",
      "📊 [embedding] 390 tokens in 0.01s → 30331.51 tokens/sec\n",
      "📊 [embedding] 322 tokens in 0.01s → 23407.90 tokens/sec\n",
      "📊 [embedding] 452 tokens in 0.01s → 31165.96 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 27320.54 tokens/sec\n",
      "📊 [embedding] 55 tokens in 0.05s → 1057.33 tokens/sec\n",
      "📊 [embedding] 376 tokens in 0.02s → 24289.71 tokens/sec\n",
      "📊 [embedding] 414 tokens in 0.01s → 27899.58 tokens/sec\n",
      "📊 [embedding] 477 tokens in 0.02s → 30626.14 tokens/sec\n",
      "📊 [embedding] 441 tokens in 0.02s → 28722.31 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34445.04 tokens/sec\n",
      "📊 [embedding] 11 tokens in 0.06s → 179.82 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31744.25 tokens/sec\n",
      "📊 [embedding] 10 tokens in 0.01s → 1214.19 tokens/sec\n",
      "📊 [embedding] 417 tokens in 0.02s → 27371.28 tokens/sec\n",
      "📊 [embedding] 487 tokens in 0.01s → 34610.81 tokens/sec\n",
      "📊 [embedding] 466 tokens in 0.01s → 35477.85 tokens/sec\n",
      "📊 [embedding] 482 tokens in 0.01s → 37197.64 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33509.93 tokens/sec\n",
      "📊 [embedding] 37 tokens in 0.05s → 726.10 tokens/sec\n",
      "📊 [embedding] 476 tokens in 0.01s → 33021.10 tokens/sec\n",
      "📊 [embedding] 415 tokens in 0.01s → 30440.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40667.70 tokens/sec\n",
      "📊 [embedding] 28 tokens in 0.05s → 569.79 tokens/sec\n",
      "📊 [embedding] 392 tokens in 0.02s → 24644.64 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31304.98 tokens/sec\n",
      "📊 [embedding] 3 tokens in 0.06s → 53.36 tokens/sec\n",
      "📊 [embedding] 398 tokens in 0.01s → 26653.89 tokens/sec\n",
      "📊 [embedding] 486 tokens in 0.02s → 32268.47 tokens/sec\n",
      "📊 [embedding] 491 tokens in 0.01s → 32746.64 tokens/sec\n",
      "📊 [embedding] 495 tokens in 0.02s → 31908.35 tokens/sec\n",
      "📊 [embedding] 435 tokens in 0.01s → 31886.65 tokens/sec\n",
      "📊 [embedding] 404 tokens in 0.02s → 21510.07 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35924.90 tokens/sec\n",
      "📊 [embedding] 18 tokens in 0.01s → 2289.47 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31669.94 tokens/sec\n",
      "📊 [embedding] 7 tokens in 0.05s → 136.04 tokens/sec\n",
      "📊 [embedding] 461 tokens in 0.02s → 30150.38 tokens/sec\n",
      "📊 [embedding] 351 tokens in 0.01s → 24623.27 tokens/sec\n",
      "📊 [embedding] 484 tokens in 0.01s → 34250.77 tokens/sec\n",
      "📊 [embedding] 388 tokens in 0.02s → 25491.30 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33487.46 tokens/sec\n",
      "📊 [embedding] 20 tokens in 0.05s → 420.69 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32197.99 tokens/sec\n",
      "📊 [embedding] 12 tokens in 0.05s → 226.89 tokens/sec\n",
      "📊 [embedding] 448 tokens in 0.02s → 28810.04 tokens/sec\n",
      "📊 [embedding] 394 tokens in 0.02s → 25401.27 tokens/sec\n",
      "📊 [embedding] 441 tokens in 0.01s → 32721.63 tokens/sec\n",
      "📊 [embedding] 442 tokens in 0.01s → 31562.43 tokens/sec\n",
      "📊 [embedding] 390 tokens in 0.02s → 25041.77 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31828.56 tokens/sec\n",
      "📊 [embedding] 32 tokens in 0.05s → 670.28 tokens/sec\n",
      "📊 [embedding] 483 tokens in 0.02s → 32003.93 tokens/sec\n",
      "📊 [embedding] 398 tokens in 0.01s → 29686.00 tokens/sec\n",
      "📊 [embedding] 438 tokens in 0.01s → 29554.46 tokens/sec\n",
      "📊 [embedding] 492 tokens in 0.01s → 37037.13 tokens/sec\n",
      "📊 [embedding] 456 tokens in 0.01s → 38393.34 tokens/sec\n",
      "📊 [embedding] 397 tokens in 0.01s → 32140.57 tokens/sec\n",
      "📊 [embedding] 496 tokens in 0.01s → 39305.75 tokens/sec\n",
      "📊 [embedding] 454 tokens in 0.01s → 37100.38 tokens/sec\n",
      "📊 [embedding] 490 tokens in 0.01s → 38320.57 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36978.98 tokens/sec\n",
      "📊 [embedding] 2 tokens in 0.01s → 230.92 tokens/sec\n",
      "📊 [embedding] 430 tokens in 0.02s → 23090.77 tokens/sec\n",
      "📊 [embedding] 492 tokens in 0.01s → 33283.83 tokens/sec\n",
      "📊 [embedding] 396 tokens in 0.01s → 27889.25 tokens/sec\n",
      "📊 [embedding] 446 tokens in 0.02s → 29140.72 tokens/sec\n",
      "📊 [embedding] 355 tokens in 0.02s → 22903.48 tokens/sec\n",
      "📊 [embedding] 446 tokens in 0.02s → 29315.63 tokens/sec\n",
      "📊 [embedding] 447 tokens in 0.01s → 34098.17 tokens/sec\n",
      "📊 [embedding] 455 tokens in 0.01s → 35613.93 tokens/sec\n",
      "📊 [embedding] 426 tokens in 0.01s → 32251.07 tokens/sec\n",
      "📊 [embedding] 370 tokens in 0.01s → 26434.09 tokens/sec\n",
      "📊 [embedding] 471 tokens in 0.01s → 34104.74 tokens/sec\n",
      "📊 [embedding] 412 tokens in 0.01s → 31276.42 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32486.79 tokens/sec\n",
      "📊 [embedding] 9 tokens in 0.01s → 1163.40 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 25241.04 tokens/sec\n",
      "📊 [embedding] 50 tokens in 0.06s → 832.61 tokens/sec\n",
      "📊 [embedding] 438 tokens in 0.02s → 22292.26 tokens/sec\n",
      "📊 [embedding] 494 tokens in 0.02s → 32696.13 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 41445.69 tokens/sec\n",
      "📊 [embedding] 28 tokens in 0.05s → 539.11 tokens/sec\n",
      "📊 [embedding] 424 tokens in 0.02s → 20663.76 tokens/sec\n",
      "📊 [embedding] 360 tokens in 0.02s → 21553.16 tokens/sec\n",
      "📊 [embedding] 494 tokens in 0.01s → 36654.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32465.16 tokens/sec\n",
      "📊 [embedding] 17 tokens in 0.01s → 1908.13 tokens/sec\n",
      "📊 [embedding] 331 tokens in 0.01s → 23605.57 tokens/sec\n",
      "📊 [embedding] 487 tokens in 0.02s → 30959.67 tokens/sec\n",
      "📊 [embedding] 431 tokens in 0.01s → 28888.79 tokens/sec\n",
      "📊 [embedding] 437 tokens in 0.02s → 26719.60 tokens/sec\n",
      "📊 [embedding] 475 tokens in 0.02s → 30372.66 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 33180.16 tokens/sec\n",
      "📊 [embedding] 13 tokens in 0.07s → 181.54 tokens/sec\n",
      "📊 [embedding] 438 tokens in 0.02s → 25223.18 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 36477.92 tokens/sec\n",
      "📊 [embedding] 39 tokens in 0.01s → 4828.44 tokens/sec\n",
      "📊 [embedding] 422 tokens in 0.01s → 28253.94 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31436.85 tokens/sec\n",
      "📊 [embedding] 8 tokens in 0.01s → 901.40 tokens/sec\n",
      "📊 [embedding] 354 tokens in 0.02s → 17280.80 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30921.87 tokens/sec\n",
      "📊 [embedding] 8 tokens in 0.06s → 131.97 tokens/sec\n",
      "📊 [embedding] 454 tokens in 0.02s → 27435.48 tokens/sec\n",
      "📊 [embedding] 380 tokens in 0.02s → 24222.79 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30684.34 tokens/sec\n",
      "📊 [embedding] 53 tokens in 0.05s → 991.29 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31490.19 tokens/sec\n",
      "📊 [embedding] 27 tokens in 0.01s → 3004.60 tokens/sec\n",
      "📊 [embedding] 439 tokens in 0.01s → 29403.40 tokens/sec\n",
      "📊 [embedding] 419 tokens in 0.02s → 25930.88 tokens/sec\n",
      "📊 [embedding] 482 tokens in 0.01s → 33556.10 tokens/sec\n",
      "📊 [embedding] 455 tokens in 0.01s → 31992.90 tokens/sec\n",
      "📊 [embedding] 309 tokens in 0.02s → 17325.58 tokens/sec\n",
      "📊 [embedding] 463 tokens in 0.01s → 32591.47 tokens/sec\n",
      "📊 [embedding] 464 tokens in 0.02s → 30718.29 tokens/sec\n",
      "📊 [embedding] 405 tokens in 0.01s → 27246.66 tokens/sec\n",
      "📊 [embedding] 387 tokens in 0.02s → 24139.25 tokens/sec\n",
      "📊 [embedding] 487 tokens in 0.01s → 34932.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 30577.41 tokens/sec\n",
      "📊 [embedding] 21 tokens in 0.06s → 361.72 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 29990.59 tokens/sec\n",
      "📊 [embedding] 10 tokens in 0.01s → 1170.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 29158.29 tokens/sec\n",
      "📊 [embedding] 28 tokens in 0.01s → 2727.94 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 31957.09 tokens/sec\n",
      "📊 [embedding] 38 tokens in 0.01s → 4402.62 tokens/sec\n",
      "📊 [embedding] 295 tokens in 0.02s → 17661.11 tokens/sec\n",
      "📊 [embedding] 379 tokens in 0.01s → 27108.02 tokens/sec\n",
      "📊 [embedding] 454 tokens in 0.02s → 29561.20 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33346.88 tokens/sec\n",
      "📊 [embedding] 20 tokens in 0.01s → 2445.02 tokens/sec\n",
      "📊 [embedding] 470 tokens in 0.01s → 32162.81 tokens/sec\n",
      "📊 [embedding] 461 tokens in 0.01s → 30753.65 tokens/sec\n",
      "📊 [embedding] 492 tokens in 0.02s → 30686.38 tokens/sec\n",
      "📊 [embedding] 240 tokens in 0.02s → 15521.76 tokens/sec\n",
      "📊 [embedding] 458 tokens in 0.01s → 31658.25 tokens/sec\n",
      "📊 [embedding] 294 tokens in 0.02s → 18652.07 tokens/sec\n",
      "📊 [embedding] 75 tokens in 0.05s → 1549.63 tokens/sec\n",
      "📊 [embedding] 391 tokens in 0.02s → 25503.83 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 35437.44 tokens/sec\n",
      "📊 [embedding] 457 tokens in 0.02s → 30180.55 tokens/sec\n",
      "📊 [embedding] 291 tokens in 0.01s → 20455.56 tokens/sec\n",
      "📊 [embedding] 192 tokens in 0.06s → 3094.04 tokens/sec\n",
      "📊 [embedding] 94 tokens in 0.01s → 11063.97 tokens/sec\n",
      "📊 [embedding] 3 tokens in 0.01s → 406.62 tokens/sec\n",
      "📊 [embedding] 424 tokens in 0.02s → 27382.52 tokens/sec\n",
      "📊 [embedding] 186 tokens in 0.06s → 3135.80 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.02s → 32265.86 tokens/sec\n",
      "📊 [embedding] 499 tokens in 0.01s → 38988.07 tokens/sec\n",
      "📊 [embedding] 78 tokens in 0.06s → 1201.54 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33415.96 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34883.93 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 37540.98 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 40488.69 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33987.84 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 33909.81 tokens/sec\n",
      "📊 [embedding] 500 tokens in 0.01s → 34671.69 tokens/sec\n",
      "📊 [embedding] 277 tokens in 0.06s → 4286.06 tokens/sec\n",
      "✅ Chunks stored in ChromaDB using local embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import ollama\n",
    "\n",
    "# ========== Embedding Setup ==========\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "performance_log = []\n",
    "\n",
    "# ========== Performance Logger ==========\n",
    "def log_performance(stage, text_or_token_list, start_time, end_time):\n",
    "    if isinstance(text_or_token_list, list) and isinstance(text_or_token_list[0], int):\n",
    "        total_tokens = len(text_or_token_list)\n",
    "    else:\n",
    "        total_tokens = len(tokenizer.encode(text_or_token_list))\n",
    "\n",
    "    elapsed = end_time - start_time\n",
    "    tokens_per_sec = total_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "    log_entry = {\n",
    "        \"stage\": stage,\n",
    "        \"tokens\": total_tokens,\n",
    "        \"duration_sec\": round(elapsed, 4),\n",
    "        \"tokens_per_sec\": round(tokens_per_sec, 2),\n",
    "        \"timestamp\": pd.Timestamp.now()\n",
    "    }\n",
    "\n",
    "    performance_log.append(log_entry)\n",
    "    print(f\"📊 [{stage}] {total_tokens} tokens in {elapsed:.2f}s → {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "    return tokens_per_sec\n",
    "\n",
    "def export_log(filename=\"performance_log.csv\"):\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ Performance log saved to {filename}\")\n",
    "\n",
    "def summarize_performance():\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    if df.empty:\n",
    "        print(\"⚠️ No performance data to summarize.\")\n",
    "        return\n",
    "    summary = df.groupby(\"stage\")[\"tokens_per_sec\"].agg([\"count\", \"min\", \"max\", \"mean\"]).reset_index()\n",
    "    print(\"\\n📈 Performance Summary:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    return summary\n",
    "\n",
    "# ========== Embedding Function ==========\n",
    "def embed_texts_local(texts):\n",
    "    start = time.time()\n",
    "    embeddings = embed_model.encode(texts).tolist()\n",
    "    end = time.time()\n",
    "    log_performance(\"embedding\", \" \".join(texts), start, end)\n",
    "    return embeddings\n",
    "\n",
    "# ========== Store to ChromaDB ==========\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}: {e}\")\n",
    "    print(\"✅ Chunks stored in ChromaDB using local embeddings.\")\n",
    "\n",
    "# ========== Ask Mistral via Ollama ==========\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    start = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    end = time.time()\n",
    "    log_performance(\"RAG\", prompt, start, end)\n",
    "    return response['message']['content']\n",
    "\n",
    "# ========== RAG Pipeline ==========\n",
    "def rag_query(question, collection_name=\"docs\", persist_dir=\"./chroma_storage\", top_k=5):\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    return ask_mistral(question, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f06998-7317-427b-b29c-b89aec72bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 [embedding] 6 tokens in 0.15s → 39.19 tokens/sec\n",
      "📊 [RAG] 1685 tokens in 106.13s → 15.88 tokens/sec\n",
      "🤖 Mistral says:\n",
      "  Here is a summary of the papers cited in the provided text:\n",
      "\n",
      "1. Polunin et al., (2001) - The study's title and details are not provided, but it is assumed to be related to stress.\n",
      "2. Stuck et al., (2001) - Again, no specifics about the study are given, but it is presumably related to stress as well.\n",
      "3. Devenport & Bax, (2002); Hoekstra et al., (2002, 2003); Nyssen et al., (2002); Sato et al., (2002) - These four studies are not specified, but they are likely to be related to stress and coping mechanisms.\n",
      "4. Schlitzer (2002); Smith et al. (2002); Bode et al. (2003, 2004, 2007); Das et al. (2003) - These studies do not have titles or specific details provided, but they are likely to be related to stress.\n",
      "5. Estrada et al. (2003); Jennings & warr (2003); Kang et al. (2003); mcclelland et al. (2003); Quay et al. (2003); Schmidt et al. (2003); corbisier et al. (2004); mahaffey et al. (2004) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "6. abed- Navandi & Dworschak  (2005); Iken et al. (2005); Kiriakoulakis et al. (2005); le loc’h & Hily (2005); Quillfeldt et al. (2005); Sommer et al. (2005); galimov et al. (2006); goni et al. (2006) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "7. Tamelander et al. (2006); carlier et al. (2007); Holl et al. (2007); cianco et al. (2008); Harmelin- vivien et al. (2008) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "8. lamb & Swart (2008); le loc’h et al. (2008); Petursdottir et al. (2008, 2010) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "9. Fanelli et al. (2009, 2011); Frederich et al. (2009); Hirch (2009); lysiak (2009) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "10. Richoux & Froneman (2009); laakmann & auel (2010); miller et al. (2010); Olson et al. (2010); Pajuelo et al. (2010) - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n",
      "11. Forest et al. (2011); Hill & mcQuaid (2011); Kohler et al. (2011); Kolasinski et al. (2011); Pomerleau et al. (2011); Stowasser - These studies do not have titles or specific details provided, but they are likely to be related to stress and coping mechanisms.\n"
     ]
    }
   ],
   "source": [
    "response = rag_query(\"summarize all papers?\")\n",
    "print(\"🤖 Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98dbf52-d72c-45a7-8b36-534b01fc1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load once\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts_local(texts):\n",
    "    \"\"\"Embed text using a local SentenceTransformer model.\"\"\"\n",
    "    return embed_model.encode(texts).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efe11e6-b680-4ae7-9fcf-85c9b73bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"drx_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}: {e}\")\n",
    "    print(\"✅ Chunks stored in ChromaDB using local embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f891cb9b-ddab-43f4-ac33-544d32928342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_local(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "    # print(results)\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"📄 {meta['file_name']} (Page {meta['page']}, Chunk {meta['chunk']})\")\n",
    "        print(doc)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a414e3b2-3f97-4775-ad88-fff8d24ad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072137e2-017a-41ee-ab51-c3731bf3f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=5):\n",
    "    # Embed the question\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    # Query ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    # Concatenate top chunks\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Generate answer with Ollama Mistral\n",
    "    answer = ask_mistral(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447aa36d-b33f-42ee-ba07-b1c8a1a79cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Mistral says:\n",
      "  This summary provides an overview of the papers cited in the course materials for the Stress Management and Paulo Coelho's books:\n",
      "\n",
      "* Polunin et al., Stuck et al., Devenport & Bax, Hoekstra et al. (2002, 2003), Nyssen et al., Sato et al., Schlitzer, Smith et al., Bode et al. (2003, 2004, 2007), Das et al., Estrada et al., Jennings & Warr, Kang et al., mcclelland et al., Quay et al., Schmidt et al., corbisier et al., mahaffey et al. (2004), abed-Navandi & Dworschak (2005), Iken et al., Kiriakoulakis et al. (2005), le loc'h & Hily (2005), Quillfeldt et al. (2005), Sommer et al. (2005), galimov et al. (2006), goni et al. (2006), Tamelander et al. (2006), carlier et al. (2007), Holl et al. (2007), cianco et al. (2008), Harmelin-vivien et al. (2008), lamb & Swart (2008), le loc'h et al. (2008), Petursdottir et al. (2008, 2010), Fanelli et al. (2009, 2011), Frederich et al. (2009), Hirch (2009), lysiak (2009), Richoux & Froneman (2009), laakmann & auel (2010), miller et al. (2010), Olson et al. (2010), Pajuelo et al. (2010), Forest et al. (2011), Hill & mcQuaid (2011), Kohler et al. (2011), Kolasinski et al. (2011), Kurten et al. (2011), Pomerleau et al. (2011), Stowasser.\n",
      "\n",
      "The papers cover various aspects of stress and coping with it, including identification of stressors, the positive and negative effects of stress, different coping styles, social support methods to mitigate stress, psychological techniques for managing stress, and case studies on stress-related issues in specific populations. These papers contribute valuable insights into stress management, providing a foundation for understanding its complexities and learning effective strategies to manage it.\n"
     ]
    }
   ],
   "source": [
    "response = rag_query(\"summarize all papers?\")\n",
    "print(\"🤖 Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5da8b2-4504-41a3-bfcf-827d526c16a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca069ad9-dfef-4dde-9c41-51e71e92e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003d2f6-a141-41e4-a1d6-12c33842a401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb892274-b3c5-49f5-899c-96aad5d43917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
