{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0385f08-033b-400f-b49b-f84a17424e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/reader/workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "üìÑ Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              file_name  page_number  chunk_number  \\\n",
      "0  Dataset summaries and citations.docx            1             1   \n",
      "1  Dataset summaries and citations.docx            1             2   \n",
      "2  Dataset summaries and citations.docx            1             3   \n",
      "3  Dataset summaries and citations.docx            1             4   \n",
      "4    Ocean_ecogeochemistry_A_review.pdf            1             1   \n",
      "\n",
      "                                                text  \n",
      "0  Table 1. Description of studies included in th...  \n",
      "1  bock, Texas. Agronomy Journal, 112(1), 148‚Äì157...  \n",
      "2  2010). Soil Organic Carbon Input from Urban Tu...  \n",
      "3   (2018). Soil carbon and nitrogen accumulation...  \n",
      "4  327\\nOceanography and Marine Biology: An Annua...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import csv\n",
    "from tqdm import tqdm  # üëà import tqdm\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens_per_chunk = 500\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return [page.get_text() for page in doc]\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return [\"\\n\".join(para.text for para in doc.paragraphs)]\n",
    "\n",
    "def extract_text_from_csv(file_path):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [f.read()]\n",
    "\n",
    "def extract_text_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path, sheet_name=None)\n",
    "    pages = []\n",
    "    for sheet, data in df.items():\n",
    "        pages.append(f\"Sheet: {sheet}\\n\" + data.to_string(index=False))\n",
    "    return pages\n",
    "\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\".csv\"]:\n",
    "        return extract_text_from_csv(file_path)\n",
    "    elif ext in [\".xls\", \".xlsx\", \".xlsm\"]:\n",
    "        return extract_text_from_excel(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {ext}\")\n",
    "        return []\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_chunks = []\n",
    "    file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    for filename in tqdm(file_list, desc=\"üìÑ Processing files\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            pages = extract_text(file_path)\n",
    "            for page_num, text in enumerate(pages, 1):\n",
    "                chunks = chunk_text(text)\n",
    "                for chunk_num, chunk in enumerate(chunks, 1):\n",
    "                    all_chunks.append({\n",
    "                        \"file_name\": filename,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_number\": chunk_num,\n",
    "                        \"text\": chunk\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "\n",
    "# üîç Usage\n",
    "folder_path = \"/Users/yasir/Desktop/Project/Dr.X Files\"\n",
    "df = process_folder(folder_path)\n",
    "print(df.head())\n",
    "df.to_csv(\"all_chunked_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10b4fee-5115-485c-b334-1975a48391b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import ollama\n",
    "import re\n",
    "from tqdm import tqdm  # ‚úÖ Added tqdm for progress bars\n",
    "\n",
    "# ========== Embedding Setup ==========\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "performance_log = []\n",
    "\n",
    "# ========== Performance Logger ==========\n",
    "def log_performance(stage, text_or_token_list, start_time, end_time):\n",
    "    if isinstance(text_or_token_list, list) and isinstance(text_or_token_list[0], int):\n",
    "        total_tokens = len(text_or_token_list)\n",
    "    else:\n",
    "        total_tokens = len(tokenizer.encode(text_or_token_list))\n",
    "\n",
    "    elapsed = end_time - start_time\n",
    "    tokens_per_sec = total_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "    log_entry = {\n",
    "        \"stage\": stage,\n",
    "        \"tokens\": total_tokens,\n",
    "        \"duration_sec\": round(elapsed, 4),\n",
    "        \"tokens_per_sec\": round(tokens_per_sec, 2),\n",
    "        \"timestamp\": pd.Timestamp.now()\n",
    "    }\n",
    "\n",
    "    performance_log.append(log_entry)\n",
    "    print(f\"üìä [{stage}] {total_tokens} tokens in {elapsed:.2f}s ‚Üí {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "    return tokens_per_sec\n",
    "\n",
    "def export_log(filename=\"performance_log.csv\"):\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Performance log saved to {filename}\")\n",
    "\n",
    "def summarize_performance():\n",
    "    df = pd.DataFrame(performance_log)\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No performance data to summarize.\")\n",
    "        return\n",
    "    summary = df.groupby(\"stage\")[\"tokens_per_sec\"].agg([\"count\", \"min\", \"max\", \"mean\"]).reset_index()\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    return summary\n",
    "\n",
    "# ========== Preprocessing DataFrame ==========\n",
    "def remove_tables_and_numeric_lines(text):\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "    tables_and_numbers = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip().lower()\n",
    "\n",
    "        # Skip short chemical/symbol lines like \"CO\", \"‚áå\", \"+\", etc.\n",
    "        if len(line.strip()) < 10 and re.match(r\"^[A-Za-z0-9()+‚àí‚áå\\\\s]*$\", line):\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "        # Remove lines like '.348136          NaN                                                              NaN'\n",
    "        if re.fullmatch(r\"[.\\d\\s]+nan\\s*nan\", line.strip().lower()):\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "        # Remove lines like '.348136          NaN    NaN' or 'NaN                  0'\n",
    "        if re.fullmatch(r\"[.\\d\\s]*nan\\s*\\d*\", line.strip().lower()):\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Skip lines that are too short and contain mostly numbers\n",
    "        if len(line.strip()) < 40 and sum(c.isdigit() for c in line) / max(len(line), 1) > 0.3:\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "        stripped = line.strip().lower()\n",
    "\n",
    "        numbers = re.findall(r\"\\d\", line)\n",
    "        if len(numbers) / max(len(line), 1) > 0.4:\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"\\t\", line) or re.search(r\"\\|\", line):\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "\n",
    "        if len(re.findall(r\"\\s{2,}\", line)) > 2:\n",
    "            tables_and_numbers.append(line)\n",
    "            continue\n",
    "\n",
    "        if any(keyword in stripped for keyword in [\"author\", \"editor\", \"publisher\", \"published by\", \"phone\", \"email\", \"contact\", \"address\"]):\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"\\b\\w+@\\w+\\.\\w+\\b\", line):\n",
    "            continue\n",
    "        if re.search(r\"\\+?\\d[\\d\\s().-]{7,}\\d\", line):\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered_lines), \"\\n\".join(tables_and_numbers)\n",
    "    \n",
    "# ========== DataFrame Cleaning ==========\n",
    "def clean_dataframe_text(df):\n",
    "    df = df.dropna(subset=['text'])  # üßπ Drop rows where 'text' is NaN\n",
    "    df = df[df['text'].str.strip().astype(bool)]  # üßπ Drop rows where 'text' is empty or only whitespace\n",
    "    df = df[df['text'].str.len() >= 10]  # üßπ Drop rows where 'text' is too short\n",
    "    df = df.dropna(subset=['text'])  # üßπ Drop rows where 'text' is NaN\n",
    "    df = df.dropna(subset=['text'])  # üßπ Drop rows where 'text' is NaN\n",
    "    df = df[df['text'].str.strip().astype(bool)]  # üßπ Drop rows where 'text' is empty or only whitespace\n",
    "    df = df[df['text'].str.strip().astype(bool)]  # üßπ Drop rows where 'text' is empty or only whitespace\n",
    "    cleaned_rows = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"üßπ Cleaning text in DataFrame\"):\n",
    "        cleaned_text, _ = remove_tables_and_numeric_lines(row['text'])\n",
    "        cleaned_rows.append({\n",
    "            \"file_name\": row['file_name'],\n",
    "            \"page_number\": row['page_number'],\n",
    "            \"chunk_number\": row['chunk_number'],\n",
    "            \"text\": cleaned_text\n",
    "        })\n",
    "        cleaned_df = pd.DataFrame(cleaned_rows)\n",
    "    cleaned_df = cleaned_df[cleaned_df['text'].str.strip().astype(bool)]  # üßπ Drop rows with empty cleaned text\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "\n",
    "# ========== Embedding Function ==========\n",
    "def embed_texts_local(texts):\n",
    "    start = time.time()\n",
    "    embeddings = embed_model.encode(texts).tolist()\n",
    "    end = time.time()\n",
    "    log_performance(\"embedding\", \" \".join(texts), start, end)\n",
    "    return embeddings\n",
    "\n",
    "# ========== Store to ChromaDB ==========\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"cleaned_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"üîÅ Storing in ChromaDB\"):\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}: {e}\")\n",
    "    print(\"‚úÖ Chunks stored in ChromaDB using local embeddings.\")\n",
    "\n",
    "# ========== Ask Mistral via Ollama ==========\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"\"Write a clear, formal, human-like summary in academic tone.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    start = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    end = time.time()\n",
    "    log_performance(\"RAG\", prompt, start, end)\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "\n",
    "# ========== RAG Pipeline ==========\n",
    "def rag_query(question, collection_name=\"cleaned_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    return ask_mistral(question, context)\n",
    "\n",
    "# ========== Summarize All Documents ==========\n",
    "def summarize_all_documents(collection_name=\"cleaned_docs\", persist_dir=\"./chroma_storage\"):\n",
    "    \"\"\"\n",
    "    Summarize all chunks grouped by document ID.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    summaries_by_file = {}\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "        file_name = meta.get(\"file_name\", \"unknown\")\n",
    "        text = doc if isinstance(doc, str) else doc[0]\n",
    "        if file_name not in summaries_by_file:\n",
    "            summaries_by_file[file_name] = []\n",
    "        summaries_by_file[file_name].append(text)\n",
    "\n",
    "    final_summaries = {}\n",
    "    for file, chunks in tqdm(list(summaries_by_file.items())[:1], desc=\"üß† Summarizing files\"):\n",
    "        combined_text = \"\\n\\n\".join(chunks)\n",
    "        question = \"\"\"\n",
    "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
    "        - Emphasize the **most important aspects**.\n",
    "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
    "        - Avoid unnecessary details while capturing the **core message**.\n",
    "        \"\"\"\n",
    "        summary = ask_mistral(question, combined_text)\n",
    "        final_summaries[file] = summary\n",
    "\n",
    "\n",
    "\n",
    "    return final_summaries\n",
    "#  # uncomment these two lines to store data in chromadb\n",
    "# clean_df= clean_dataframe_text(df) \n",
    "# store_df_in_chromadb_local(clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9714b028-beec-4c76-83e0-b530ee029bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ========== Evaluate Summaries with ROUGE ==========\n",
    "def evaluate_summaries_with_rouge(generated_summaries: dict, reference_summaries: dict):\n",
    "    \"\"\"\n",
    "    Compare generated summaries to reference summaries using ROUGE scores.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    evaluation = {}\n",
    "\n",
    "    for doc_id, generated in tqdm(generated_summaries.items(), desc=\"üìè Evaluating ROUGE\"):\n",
    "        reference = reference_summaries.get(doc_id, \"\")\n",
    "        if not reference:\n",
    "            print(f\"‚ö†Ô∏è No reference summary found for document: {doc_id}\")\n",
    "            continue\n",
    "\n",
    "        scores = scorer.score(generated, reference)\n",
    "        evaluation[doc_id] = {\n",
    "            metric: {\n",
    "                \"precision\": round(scores[metric].precision, 4),\n",
    "                \"recall\": round(scores[metric].recall, 4),\n",
    "                \"f1\": round(scores[metric].fmeasure, 4)\n",
    "            } for metric in scores\n",
    "        }\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a410b7b-45c4-46e3-839a-e99cf7cef764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Preloaded Reference Summaries ==========\n",
    "reference_summaries = {\n",
    "    \"Dataset summaries and citations.docx\": \"\"\"\n",
    "    1. Numerous studies have investigated carbon accumulation and nitrogen cycling in various urban land uses including residential soils, golf courses, and home lawns.\n",
    "    2. Some research emphasizes specific biophysical factors influencing soil carbon, such as:\n",
    "       - Microbial processes (e.g., Shi et al., 2012),\n",
    "       - Management intensity and duration (e.g., Wang et al., 2014),\n",
    "       - Historical land use changes (e.g., Raccanello et al., 2011).\n",
    "    3. A subset of studies analyzes regional influences on carbon sequestration, examining how soil characteristics and climatic conditions affect carbon dynamics in urban ecosystems (e.g., Selhorst & Lal, 2011; Smith et al., 2018).\n",
    "    4. Comparative assessments have been conducted to evaluate differences in carbon sequestration across urban land typologies, including turfgrass systems and residential zones with varied development histories.\n",
    "    5. Findings indicate that urban landscapes can serve as significant carbon sinks, occasionally surpassing the sequestration potential of rural or forested lands (Selhorst & Lal, 2011; Trammell et al., 2020).\n",
    "    6. Despite their potential benefits, urban environments also contribute to greenhouse gas emissions, especially from impervious surfaces and vehicular traffic (Townsend-Small & Czimczik, 2010).\n",
    "    7. Research suggests that residential lawns have a variable carbon sequestration potential, heavily influenced by climate, management practices, and underlying soil properties.\n",
    "    8. In conclusion, while urban landscapes offer valuable opportunities for climate change mitigation through carbon storage, maximizing their potential requires integrated land management strategies that simultaneously limit greenhouse gas emissions.\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98874883-a890-43fa-93a3-801edf26aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "# üîë Generate or retrieve session ID\n",
    "session_id = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"_\" + str(uuid.uuid4())[:8]\n",
    "\n",
    "persist_dir=\"./chroma_storage\"\n",
    "chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "collection_chat_history = chroma_client.get_or_create_collection(\"chat_history\")\n",
    "\n",
    "def get_full_chat_history(session_id):\n",
    "    \n",
    "    results = collection_chat_history.get(\n",
    "        where={\"session_id\": session_id},\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    documents = results.get(\"documents\", [])\n",
    "    # Sort by timestamp if available\n",
    "    metadatas = results.get(\"metadatas\", [])\n",
    "    if metadatas and \"timestamp\" in metadatas[0]:\n",
    "        combined = sorted(zip(documents, metadatas), key=lambda x: x[1][\"timestamp\"])\n",
    "        return [doc for doc, _ in combined]\n",
    "    return documents\n",
    "\n",
    "\n",
    "def store_chat_turn(session_id, user_msg, bot_msg):\n",
    "    full_turn = f\"User: {user_msg}\\nAssistant: {bot_msg}\"\n",
    "    embedding = embed_model.encode(full_turn).tolist()\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    collection_chat_history.add(\n",
    "        documents=[full_turn],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id],\n",
    "        metadatas=[{\n",
    "            \"session_id\": session_id,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }]\n",
    "    )\n",
    "def get_chat_history(session_id, top_k=5):\n",
    "    results = collection_chat_history.query(\n",
    "        query_texts=[\"latest topic continuation\"],  # or current user query\n",
    "        n_results=top_k,\n",
    "        where={\"session_id\": session_id}\n",
    "    )\n",
    "    return results[\"documents\"][0]  # list of retrieved turns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc75ab1-f5d5-4e02-97d0-d160de9022c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [RAG] 1848 tokens in 42.88s ‚Üí 43.10 tokens/sec\n",
      "User: Summarize all documents\n",
      "Assistant:  The summarized account of the provided documents indicates a comprehensive investigation into the effects of climate change on various ecosystems and potential solutions to mitigate its impact. The documents are categorized as follows:\n",
      "\n",
      "1. Document A discusses the significance of mangroves in carbon sequestration and their role in reducing greenhouse gas emissions, emphasizing their importance in maintaining coastal ecosystem health and combating climate change.\n",
      "\n",
      "2. Document B examines the effects of increasing temperatures on terrestrial ecosystems, focusing on changes in species distribution patterns, shifts in plant phenology, and potential risks to biodiversity due to habitat destruction.\n",
      "\n",
      "3. Document C explores renewable energy technologies as a means of reducing fossil fuel consumption, discussing their applications, advantages, challenges, and potential for widespread adoption in mitigating climate change.\n",
      "\n",
      "4. Document D examines the role of sustainable agriculture practices in carbon sequestration, addressing methods such as no-till farming, crop rotation, and agroforestry, and their impact on overall carbon emissions reductions.\n",
      "\n",
      "5. Document E delves into the potential of geoengineering techniques to combat climate change, discussing ocean fertilization, carbon capture and storage, and solar radiation management, as well as their ethical implications and risks.\n",
      "\n",
      "The documents collectively suggest that a combination of strategies involving renewable energy adoption, sustainable agricultural practices, mangrove conservation, and careful consideration of geoengineering techniques is necessary to address the complex issue of climate change effectively.\n",
      "\n",
      "User: Summarize all documents\n",
      "Assistant:  The provided set of documents offers an in-depth exploration of various aspects related to climate change and potential strategies for mitigation. Specifically, the following topics are addressed:\n",
      "\n",
      "1. Document A highlights the role of mangroves in carbon sequestration, stressing their importance in reducing greenhouse gas emissions and preserving coastal ecosystem health as a means of combating climate change.\n",
      "\n",
      "2. Document B discusses the impacts of rising temperatures on terrestrial ecosystems, focusing on alterations in species distribution patterns, adjustments in plant phenology, and the potential risks to biodiversity due to habitat destruction.\n",
      "\n",
      "3. Document C investigates renewable energy technologies as a solution for reducing fossil fuel consumption, discussing their applications, advantages, challenges, and the potential for broader implementation in combating climate change.\n",
      "\n",
      "4. Document D examines sustainable agriculture practices that aid in carbon sequestration, focusing on techniques such as no-till farming, crop rotation, and agroforestry, and evaluates their impact on overall reductions in carbon emissions.\n",
      "\n",
      "5. Document E delves into geoengineering techniques as potential climate change solutions, discussing options like ocean fertilization, carbon capture and storage, and solar radiation management while considering the ethical implications and risks associated with each method.\n",
      "\n",
      "In conclusion, these documents collectively propose that a multipronged approach involving the adoption of renewable energy, sustainable agricultural practices, mangrove conservation, and careful examination of geoengineering techniques is essential for an effective response to climate change.\n",
      "\n",
      "User: Summarize all documents\n",
      "Assistant:  The set of provided documents presents a thorough investigation into the multifaceted issue of climate change and proposes various strategies for mitigation. Topics discussed within these documents include:\n",
      "\n",
      "1. Document A delves into the vital role mangroves play in carbon sequestration, emphasizing their importance in reducing greenhouse gas emissions and preserving the health of coastal ecosystems.\n",
      "\n",
      "2. Document B discusses the effects of increasing temperatures on terrestrial ecosystems, with a focus on changes in species distribution patterns, shifts in plant phenology, and potential risks to biodiversity due to habitat destruction.\n",
      "\n",
      "3. Document C examines renewable energy technologies as a means of reducing fossil fuel consumption, focusing on their applications, advantages, challenges, and the potential for broader implementation in combating climate change.\n",
      "\n",
      "4. Document D investigates sustainable agriculture practices that aid in carbon sequestration, highlighting techniques such as no-till farming, crop rotation, and agroforestry, and evaluating their impact on overall reductions in carbon emissions.\n",
      "\n",
      "5. Document E delves into geoengineering techniques as potential climate change solutions, discussing options like ocean fertilization, carbon capture and storage, and solar radiation management while considering the ethical implications and risks associated with each method.\n",
      "\n",
      "In summation, these documents collectively suggest that a multipronged approach involving the adoption of renewable energy, sustainable agricultural practices, mangrove conservation, and careful examination of geoengineering techniques is essential for an effective response to climate change.\n",
      "\n",
      "User: \n",
      "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
      "        - Emphasize the **most important aspects**.\n",
      "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
      "        - Avoid unnecessary details while capturing the **core message**.\n",
      "        \n",
      "Assistant: 1. Role of mangroves in carbon sequestration and their importance for coastal ecosystem health and combating climate change.\n",
      "\n",
      "2. Impacts of rising temperatures on terrestrial ecosystems, focusing on changes in species distribution patterns, adjustments in plant phenology, and potential risks to biodiversity due to habitat destruction.\n",
      "\n",
      "3. Investigation into renewable energy technologies as a solution for reducing fossil fuel consumption, discussing their applications, advantages, challenges, and the potential for broader implementation in combating climate change.\n",
      "\n",
      "4. Examination of sustainable agriculture practices that aid in carbon sequestration, focusing on techniques such as no-till farming, crop rotation, and agroforestry, and evaluating their impact on overall reductions in carbon emissions.\n",
      "\n",
      "5. Exploration of geoengineering techniques as potential climate change solutions, discussing options like ocean fertilization, carbon capture and storage, and solar radiation management while considering the ethical implications and risks associated with each method.\n",
      "\n",
      "In conclusion, a multipronged approach involving renewable energy adoption, sustainable agricultural practices, mangrove conservation, and careful examination of geoengineering techniques is essential for an effective response to climate change.\n",
      "\n",
      "User: \n",
      "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
      "        - Emphasize the **most important aspects**.\n",
      "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
      "        - Avoid unnecessary details while capturing the **core message**.\n",
      "        \n",
      "Assistant: 1. Investigation into mangrove roles in carbon sequestration, their significance for coastal ecosystem health, and combating climate change.\n",
      "2. Analysis of the impacts of rising temperatures on terrestrial ecosystems, with a focus on changes in species distribution patterns, adjustments in plant phenology, and potential risks to biodiversity due to habitat destruction.\n",
      "3. Examination of renewable energy technologies as a solution for reducing fossil fuel consumption, emphasizing their applications, advantages, challenges, and the potential for broader implementation in combating climate change.\n",
      "4. Study of sustainable agriculture practices that aid in carbon sequestration, highlighting techniques such as no-till farming, crop rotation, and agroforestry, and evaluating their impact on overall reductions in carbon emissions.\n",
      "5. Exploration of geoengineering techniques as potential climate change solutions, discussing options like ocean fertilization, carbon capture and storage, and solar radiation management while considering the ethical implications and risks associated with each method.\n",
      "6. Collectively suggests that a multipronged approach involving renewable energy adoption, sustainable agricultural practices, mangrove conservation, and careful examination of geoengineering techniques is essential for an effective response to climate change.\n",
      "\n",
      "User: \n",
      "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
      "        - Emphasize the **most important aspects**.\n",
      "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
      "        - Avoid unnecessary details while capturing the **core message**.\n",
      "        \n",
      "Assistant: 1. Importance of mangroves in carbon sequestration, reducing greenhouse gas emissions, and maintaining coastal ecosystem health to combat climate change.\n",
      "\n",
      "2. Impacts of rising temperatures on terrestrial ecosystems, affecting species distribution patterns, plant phenology, and biodiversity due to habitat destruction.\n",
      "\n",
      "3. Examination of renewable energy technologies as a means of reducing fossil fuel consumption, highlighting applications, advantages, challenges, and potential for broader implementation in climate change mitigation.\n",
      "\n",
      "4. Study of sustainable agriculture practices that aid carbon sequestration, focusing on no-till farming, crop rotation, agroforestry, and their impact on overall reductions in carbon emissions.\n",
      "\n",
      "5. Exploration of geoengineering techniques as potential climate change solutions, discussing ocean fertilization, carbon capture and storage, solar radiation management, and the associated ethical implications and risks.\n",
      "\n",
      "6. Essentiality of a multipronged approach involving renewable energy adoption, sustainable agricultural practices, mangrove conservation, and careful examination of geoengineering techniques for an effective response to climate change.\n",
      "User: \n",
      "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
      "        - Emphasize the **most important aspects**.\n",
      "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
      "        - Avoid unnecessary details while capturing the **core message**.\n",
      "        \n",
      "Assistant:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìè Evaluating ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ROUGE Evaluation for Final Refined Summary:\n",
      "ROUGE1     Precision: 0.2423     Recall: 0.3716     F1: 0.2933    \n",
      "ROUGE2     Precision: 0.0310     Recall: 0.0476     F1: 0.0375    \n",
      "ROUGEL     Precision: 0.1101     Recall: 0.1689     F1: 0.1333    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üß† Retrieve full session chat\n",
    "chat_history_snippets = get_full_chat_history(session_id)\n",
    "full_context = \"\\n\\n\".join(chat_history_snippets)\n",
    "question = \"\"\"\n",
    "        - Summarize using concise **bullet points** or **brief paragraphs**.\n",
    "        - Emphasize the **most important aspects**.\n",
    "        - Maintain an **academic tone** and ensure **clarity** throughout.\n",
    "        - Avoid unnecessary details while capturing the **core message**.\n",
    "        \"\"\"\n",
    "# ü§ñ Generate refined summary\n",
    "prompt = f\"{full_context}\\nUser: {question}\\nAssistant:\"\n",
    "response = ask_mistral(question, prompt)\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "# store chat history\n",
    "store_chat_turn(session_id,question, response)\n",
    "\n",
    "# üìù Wrap the response for ROUGE evaluation\n",
    "filename = \"Dataset summaries and citations.docx\"  # or dynamically match the file\n",
    "refined_summary = {filename: response}\n",
    "\n",
    "# üìä Evaluate the new summary\n",
    "rouge_result = evaluate_summaries_with_rouge(refined_summary, reference_summaries)\n",
    "\n",
    "# üì¢ Display\n",
    "print(f\"\\nüìä ROUGE Evaluation for Final Refined Summary:\")\n",
    "for metric, values in rouge_result[filename].items():\n",
    "    print(f\"{metric.upper():<10} Precision: {values['precision']:<10.4f} Recall: {values['recall']:<10.4f} F1: {values['f1']:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6c9b4-aaac-4c0d-9c46-9bcbab48db71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a6d2d-281d-41b9-bf6b-2617e461e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffdd9f-c3a7-48a3-90a9-1592f09cbc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e5704-497d-494a-a4ca-c3858d940a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6536e37-a4ac-4615-b687-0359254d34d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5abf44-2251-49a1-9b38-9b45d53884db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------xxxxxxxxxxxx-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e13c95f-23bb-4c70-ab68-8d6b2f1de756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== Example Evaluation Run ==========\n",
    "\n",
    "\n",
    "generated_summaries = summarize_all_documents()\n",
    "\n",
    "results = evaluate_summaries_with_rouge(res, reference_summaries)\n",
    "question= \"Summarize all documents\"\n",
    "store_chat_turn(session_id,question, generated_summaries)\n",
    "for filename, scores in results.items():\n",
    "    print(f\"\\nROUGE Scores for: {filename}\\n\")\n",
    "    print(f\"{'Metric':<10} {'Precision':<10} {'Recall':<10} {'F1 Score':<10}\")\n",
    "    for metric, values in scores.items():\n",
    "        print(f\"{metric.upper():<10} {values['precision']:<10.4f} {values['recall']:<10.4f} {values['f1']:<10.4f}\")\n",
    "context_snippets = get_chat_history(session_id)\n",
    "full_context = \"\\n\".join(context_snippets)\n",
    "prompt = f\"{full_context}\\nUser: {question}\\nAssistant:\"\n",
    "# res = ask_mistral(question, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11a8ab-090c-40b0-a7d4-006bc40ccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_documents(collection_name=\"docs\", persist_dir=\"./chroma_storage\", max_chunks=5):\n",
    "    \"\"\"\n",
    "    Summarize a limited number of stored chunks to reduce load and return a list of summaries.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.get(include=[\"documents\"])\n",
    "    all_chunks = results.get(\"documents\", [])\n",
    "\n",
    "    print(f\"üß† Summarizing up to {max_chunks} chunks...\")\n",
    "    summaries = []\n",
    "    for idx, chunk in enumerate(all_chunks[:max_chunks]):\n",
    "        if isinstance(chunk, str) and chunk.strip():\n",
    "            summary = ask_mistral(\"Summarize the following document chunk.\", chunk)\n",
    "            summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dbf52-d72c-45a7-8b36-534b01fc1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load once\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts_local(texts):\n",
    "    \"\"\"Embed text using a local SentenceTransformer model.\"\"\"\n",
    "    return embed_model.encode(texts).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe11e6-b680-4ae7-9fcf-85c9b73bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "def store_df_in_chromadb_local(df, persist_dir=\"./chroma_storage\", collection_name=\"drx_docs\"):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = f\"{row['file_name']}_{row['page_number']}_{row['chunk_number']}\"\n",
    "        try:\n",
    "            embedding = embed_texts_local([row['text']])[0]\n",
    "            collection.add(\n",
    "                documents=[row['text']],\n",
    "                ids=[doc_id],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": row['file_name'],\n",
    "                    \"page\": row['page_number'],\n",
    "                    \"chunk\": row['chunk_number']\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}: {e}\")\n",
    "    print(\"‚úÖ Chunks stored in ChromaDB using local embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891cb9b-ddab-43f4-ac33-544d32928342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_local(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=1):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "    # print(results)\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"üìÑ {meta['file_name']} (Page {meta['page']}, Chunk {meta['chunk']})\")\n",
    "        print(doc)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414e3b2-3f97-4775-ad88-fff8d24ad777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_mistral(question, context):\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072137e2-017a-41ee-ab51-c3731bf3f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, collection_name=\"drx_docs\", persist_dir=\"./chroma_storage\", top_k=5):\n",
    "    # Embed the question\n",
    "    query_vector = embed_texts_local([question])[0]\n",
    "\n",
    "    # Query ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=top_k)\n",
    "\n",
    "    # Concatenate top chunks\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Generate answer with Ollama Mistral\n",
    "    answer = ask_mistral(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447aa36d-b33f-42ee-ba07-b1c8a1a79cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_query(\"summarize all papers?\")\n",
    "print(\"ü§ñ Mistral says:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5da8b2-4504-41a3-bfcf-827d526c16a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca069ad9-dfef-4dde-9c41-51e71e92e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003d2f6-a141-41e4-a1d6-12c33842a401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb892274-b3c5-49f5-899c-96aad5d43917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
